{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Workshop 6\n",
    "## Pytorch Fundamentals + Logistic Regression / Perceptron revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pytorch](https://pytorch.org/) is a open-source Python library designed for fast matrix computations on CPU/GPU. This includes both standard linear algebra and deep learning-specific operations. It is based on the neural network backend of the [Torch library](http://torch.ch/). A central feature of Pytorch is its use of Automatic on-the-fly differentiation ([Autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)) to compute derivatives of (almost) all computations involving tensors, so we can make use of gradient-based updates to optimize some objective function. In this wokkshop we will introduce some fundamental operations in Pytorch and reimplement the Perceptron and logistic regression classifiers in Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pytorch Computational Model\n",
    "\n",
    "To begin, let's take a look at the result of the same computation in Tensorflow and Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrices with entries drawn from the standard normal, and multiply them\n",
    "x_tf = tf.random.normal(shape=[4,4], seed=90051)\n",
    "y_tf = tf.random.normal(shape=[4,8], seed=90051)\n",
    "z_tf = tf.matmul(x_tf,y_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul:0' shape=(4, 8) dtype=float32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤”\n",
    "Why isn't this returning the result of our computation? Unlike normal Python, this doesn't return the value of the computation, but a pointer to a node embedded in a graph structure representing this computation. \n",
    "\n",
    "Working in Tensorflow requires us to do some metaprogramming - we must first declare the tensors and associated operations then execute the computation at some later stage. This is known as a _static graph_ framework - operations represent _nodes_ while tensors are _edges_ in a computational graph structure. Working 'twice removed' from our results can be confusing, especially in complex models which use conditionals such as `if` and `while`. One must construct a `tf.Session` object to link the pre-built graph to the `C++` runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "z_exec = sess.run(z_tf)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_exec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: [Tensorflow 2.0](https://www.tensorflow.org/beta) will be moving away from the static graph framework to support dynamic execution by default.\n",
    "Let's look at the same operation in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "# Create matrices with entries drawn from the standard normal, and multiply them\n",
    "x_pt = torch.randn(size=[4,4])\n",
    "y_pt = torch.randn(size=[4,8])\n",
    "z_pt = torch.matmul(x_pt, y_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like regular Python, the result is returned immediately! Behind the scenes, each time we evaluate a Pytorch operation, the library silently defines a new computation graph which is executed immediately, rather than deferred to some later stage. This usually makes debugging easier (but no always!) and supports native Python control flow and iteration. For more discussion of the pros and cons of static vs. dynamic graphs one may refer to https://arxiv.org/abs/1701.03980. \n",
    "\n",
    "*Discussion:* Which framework do you think is more efficient?\n",
    "\n",
    "*Answer:* One big point of contention is the relative speed of both approaches. In modern applications, the network parameters will always remain on the GPU during computation. The network structure can be thought of as metadata that tells the GPU which operations to perform in which order. Generating this metadata dynamically typically incurs a negligible overhead relative to the actual matrix computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Ops\n",
    "\n",
    "We will not spend much time covering basic operations here. The basic API is extremely similar to `numpy`. One may consult the [excellent introduction at the official Pytorch repository](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html). \n",
    "\n",
    "* The fundamental object in Pytorch is the Tensor. This is a generalized matrix - essentially an $n$-dimensional table of numerical values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([2,7,1,8,2])\n",
    "print(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One may perform the basic mathematical operations expected on Tensors. In future weeks we will see that 3D tensors can be used to represent a sequence of identically sized 2D matrices - or a multichannel image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(size=[3,3,3])\n",
    "for i in range(x.shape[0]):\n",
    "    print(x[i,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Similarly, 4D tensors can be used to represent a sequence of 3D tensors, or a matrix of identically sized matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(size=[2,2,2,2])\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    for j in range(x.shape[1]):\n",
    "        print(x[i,j,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can easily convert between `numpy` arrays and `torch` tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones(9)*2\n",
    "x_pt = torch.from_numpy(x)\n",
    "print(x)\n",
    "print(x_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pt = torch.pow(x_pt,8)\n",
    "# Create a new 'view' of the same underlying 1D vector of \n",
    "# numbers stored in memory - similar to np.reshape\n",
    "x_pt = x_pt.view([3,3])  \n",
    "x = x_pt.numpy()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "To compute the gradients of tensors with respect to other tensors, a full record of operations performed on the Tensor must be retained. This can be achieved by setting the attribute `.requires_grad = True`. After the computation is finished, calling `.backward()` on the tensor walks through the computation history in the backwards order to automatically compute the gradients using a method based on the chain rule, which can be used for optimization of arbitrary loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0\n",
    "x = torch.ones([3,3], requires_grad=True)\n",
    "loss = torch.exp(-(x-mu)**2/(2)).mean()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient for the tensor `x` is accumulated into the `.grad` attribute. We would use this in some gradient based update rule, such as vanilla SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we get the gradients right? Using the chain rule to calculate the gradients of the loss $l$ with respect to $x$ analytically, we find:\n",
    "\n",
    "$$ l = \\frac{1}{9} \\exp\\left[-\\frac{(x-\\mu)^2}{2}\\right] \\implies \\frac{dl}{dx} = - \\frac{1}{9}(x-\\mu)\\exp\\left[-\\frac{(x-\\mu)^2}{2}\\right] \\$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_dx = lambda x: - 1/9 * (x-mu) * np.exp(-(x-mu)**2/2)\n",
    "print(dl_dx(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the analytic gradient and the autograd-computed gradient agree - but autograd can be very useful when evaluating the derivatives of loss functions constructed as a sequence of nested operations (an example is the cross-entropy loss when using a neural network for classification) where the analytic derivative is difficult to compute symbolically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron, reborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the remaining part of this workshop we'll look at two different ways of implementing the Perceptron in Pytorch. The first is the conditional approach you encountered last week and the second employs gradient descent on the Perceptron objective function. First we'll generate the data as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "import time, os\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def create_toy_data(n_samples=250, class_sep=2):\n",
    "    X, Y = make_classification(n_samples=n_samples, n_features=2, n_informative=2, \n",
    "                               n_redundant=0, n_clusters_per_class=1, flip_y=0,\n",
    "                               class_sep=class_sep, random_state=1)\n",
    "    Y[Y==0] = -1  # encode \"negative\" class using -1 rather than 0\n",
    "    plt.plot(X[Y==-1,0], X[Y==-1,1], \".\", label=\"$y = -1$\")\n",
    "    plt.plot(X[Y==1,0], X[Y==1,1], \".\", label=\"$y = 1$\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"$x_0$\")\n",
    "    plt.ylabel(\"$x_1$\")\n",
    "    plt.show()\n",
    "    \n",
    "    return X.astype(np.float32), Y.astype(np.float32)\n",
    "\n",
    "def plot_results(X_train, Y_train, X_test, Y_test, score_fn, threshold = 0):\n",
    "    # Plot training set\n",
    "    plt.plot(X_train[Y_train==-1,0], X_train[Y_train==-1,1], \".\", label=r\"$y=-1$, train\")\n",
    "    plt.plot(X_train[Y_train==1,0], X_train[Y_train==1,1], \".\", label=r\"$y=1$, train\")\n",
    "    plt.gca().set_prop_cycle(None) # reset colour cycle\n",
    "\n",
    "    # Plot test set\n",
    "    plt.plot(X_test[Y_test==-1,0], X_test[Y_test==-1,1], \"x\", label=r\"$y=-1$, test\")\n",
    "    plt.plot(X_test[Y_test==1,0], X_test[Y_test==1,1], \"x\", label=r\"$y=1$, test\")\n",
    "\n",
    "    # Compute axes limits\n",
    "    border = 1\n",
    "    x0_lower = X[:,0].min() - border\n",
    "    x0_upper = X[:,0].max() + border\n",
    "    x1_lower = X[:,1].min() - border\n",
    "    x1_upper = X[:,1].max() + border\n",
    "\n",
    "    # Generate grid over feature space\n",
    "    resolution = 0.01\n",
    "    x0, x1 = np.mgrid[x0_lower:x0_upper:resolution, x1_lower:x1_upper:resolution]\n",
    "    grid = np.c_[x0.ravel(), x1.ravel()]\n",
    "    s = score_fn(grid).reshape(x0.shape)\n",
    "\n",
    "    # Plot decision boundary (where s(x) == 0)\n",
    "    plt.contour(x0, x1, s, levels=[0], cmap=\"Greys\", vmin=-0.2, vmax=0.2)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"$x_0$\")\n",
    "    plt.ylabel(\"$x_1$\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_RESOLUTION = 128\n",
    "plt.rcParams['figure.dpi'] = FIGURE_RESOLUTION\n",
    "X, Y = create_toy_data(class_sep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=90051)\n",
    "print(\"Training set has {} instances. Test set has {} instances.\".format(X_train.shape[0], X_test.shape[0]))\n",
    "x_train, y_train = torch.from_numpy(X_train), torch.from_numpy(Y_train)\n",
    "x_test, y_test = torch.from_numpy(X_test), torch.from_numpy(Y_test)\n",
    "\n",
    "add_bias_column = lambda x: torch.cat([torch.ones(x.shape[0],1), x], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we'll re-implement the Perceptron update rule from last week in Torch. To simplify matters, we'll include the bias term in the weight vector and prepend a column of ones to the feature matrix to account for this. \n",
    "\n",
    "<img src=\"https://i.imgur.com/N6NZrAC.png\" alt=\"Perceptron alg.\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(x, y, n_epochs=10, eta=0.1):\n",
    "    \n",
    "    x = add_bias_column(x)\n",
    "    w = torch.zeros(x.shape[1])\n",
    "    errors = 0\n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        for i in range(x.size(0)):\n",
    "            if ... <= 0:\n",
    "                deltaW = ...  # fill in\n",
    "                ...\n",
    "            \n",
    "        if errors == 0: break;\n",
    "            \n",
    "    print('Errors: {} / {} Iterations'.format(errors, n_epochs*x.shape[0]))\n",
    "    print('Trained weights:', w)\n",
    "    return w.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = train_perceptron(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(X_train, Y_train, X_test, Y_test, score_fn=lambda x: np.dot(x,w[1:])+w[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By design, the above update scheme is equivalent to sequential gradient descent on the following objective function:\n",
    "\n",
    "\\begin{align}\n",
    "    F(\\mathbf{w}) &= \\frac{1}{T} \\sum_{t=1}^T \\max\\left(0, -y_t (\\mathbf{w} \\cdot \\mathbf{x}_t)\\right) \\\\\n",
    "    \\mathbf{w}_{t+1} &\\leftarrow \\mathbf{w}_t - \\eta \\nabla_{\\mathbf{w}} F(\\mathbf{w}) \n",
    "\\end{align}\n",
    "\n",
    "Below we'll use the autograd mechanics of Pytorch to implement this update scheme. The basic optimization strategy in Torch is as follows:\n",
    "1. Define parameters to be optimized (in this case, $\\mathbf{w}$)\n",
    "2. Define an update rule through an `optimizer` (in this case, sequential gradient descent using `torch.optim.SGD`)\n",
    "3. Start the training loop:\n",
    "    * Retrieve your input tensors `x`\n",
    "    * Compute your model output tensors `out` in the forward pass: `out = my_fancy_model(x)`\n",
    "    * Compute the loss at each iteration. (`loss = my_loss_function(out)`)\n",
    "    * Find the gradient of the loss with respect to trainable model parameters $\\mathbf{w}$ through autograd. (`loss.backward()`)\n",
    "    * Take a gradient step to minimize the loss (`optimizer.step()`)\n",
    "    * Reset accumulated gradients to zero for the next iteration (`optimizer.zero_grad()`). This step is needed as Pytorch by default accumulates gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_loss(x,y,w):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        w: Trainable weight vector\n",
    "           Torch tensor, of shape [n_features]\n",
    "        x: Feature matrix\n",
    "           Torch tensor, of shape [batch_size, n_features]\n",
    "        y: Truth labels\n",
    "           Torch tensor, of shape [batch_size]\n",
    "    \"\"\"\n",
    "    # Remove trailing '1' dimensions\n",
    "    x = torch.squeeze(x)\n",
    "    \n",
    "    flag = -y * (torch.dot(w,x))\n",
    "    loss = torch.max(torch.Tensor([0]), flag)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron_SGD(x, y, n_epochs=10, eta=0.1):\n",
    "    \n",
    "    # Create iterable dataset in Torch format - technical details, \n",
    "    # don't worry too much about this!\n",
    "    x = add_bias_column(x)\n",
    "    train_ds = torch.utils.data.TensorDataset(x, y)\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=1)\n",
    "    \n",
    "    # Create trainable weight vector, tell Torch to track operations on w\n",
    "    w = torch.zeros(x.shape[1], requires_grad=True)\n",
    "    \n",
    "    # Setup the optimizer. This implements the basic gradient descent update\n",
    "    optimizer = torch.optim.SGD([w], lr=eta)\n",
    "    \n",
    "    for _ in range(n_epochs):\n",
    "        for i, (x,y) in enumerate(train_loader):\n",
    "            x = torch.squeeze(x)\n",
    "            \n",
    "            # Compute loss F(w)\n",
    "            loss = perceptron_loss(x,y,w)\n",
    "            \n",
    "            loss.backward()               # Backward pass (compute parameter gradients)\n",
    "            optimizer.step()              # Update weight parameter using SGD\n",
    "            optimizer.zero_grad()         # Reset gradients to zero for next ieration\n",
    "        \n",
    "    w = w.detach()  # Tell Torch to stop tracking operations for autograd\n",
    "    print('Trained weights:', w)\n",
    "    return w.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = train_perceptron_SGD(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(X_train, Y_train, X_test, Y_test, score_fn=lambda x: np.dot(x,w[1:])+w[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we get the same results. While there is no discernable advantage here in using autograd mechanics to compute gradient-based updates, we will now take a look at an image classification example where the analytic gradient is intractable. In such situations we must rely on automatic differentiation to compute the gradients of the loss with respect to the parameters needed for backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression\n",
    "\n",
    "Remember two weeks ago when there was a question on how to generalize binary logistic regression to more than two classes? This is why it pays to do the bonus section. We briefly repeat the argument here, for more details one can refer to the solutions on Blackboard.\n",
    "\n",
    "We would like to output an $m$-dimensional vector of conditional class probabilities $(p_1, p_2, \\ldots, p_m)$. We require $p_k \\in [0,1]$ and $\\sum_k p_k=1$ by the law of total probability. Taking inspiration from the logistic regression case, we can achieve this by exponentiating the output of our classifier $f(\\mathbf{x}) = W^T \\mathbf{\\Phi}(\\mathbf{x}) = \\left[\\mathbf{w}_0^T\\mathbf{\\Phi} \\vert \\ldots \\vert \\mathbf{w}_m^T\\mathbf{\\Phi}\\right]$, where $\\mathbf{\\Phi}: \\mathbb{R}^d \\rightarrow \\mathbb{R}^D$ is some possibly nonlinear transformation typically mapping the instance $\\mathbf{x} \\in \\mathbb{R}^d$ to some higher-dimensional space, and $\\mathbf{w} \\in \\mathbb{R}^D$, $W \\in \\mathbb{R}^{D \\times m}$. In essence we have multiple weight vectors $(\\mathbf{w}_1, \\ldots \\mathbf{w}_m)$, one corresponding to each class, and the output $\\mathbf{w}_k \\cdot \\mathbf{\\Phi}(\\mathbf{x})$ 'scores' how much the classifier thinks the instance $\\mathbf{x}$ belongs to class $k$. More concretely, the matrix operation looks like:\n",
    "\n",
    "\\begin{equation}\n",
    "    W^T \\mathbf{\\Phi} = \\begin{bmatrix}\n",
    "      \\leftarrow \\mathbf{w}^{T}_{0} \\rightarrow \\\\\n",
    "      \\leftarrow \\mathbf{w}^{T}_{1} \\rightarrow \\\\\n",
    "      \\vdots \\\\\n",
    "      \\leftarrow \\mathbf{w}^{T}_{m} \\rightarrow \\\\ \n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "      \\mathbf{\\Phi}^{(1)}  \\\\\n",
    "      \\mathbf{\\Phi}^{(2)}  \\\\\n",
    "      \\vdots \\\\\n",
    "      \\mathbf{\\Phi}^{(D)}\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "      \\mathbf{w}_0 \\cdot \\mathbf{\\Phi}  \\\\\n",
    "      \\mathbf{w}_1 \\cdot \\mathbf{\\Phi}  \\\\\n",
    "      \\vdots \\\\\n",
    "      \\mathbf{w}_m \\cdot \\mathbf{\\Phi}\n",
    "    \\end{bmatrix} \\in \\mathbb{R}^m\n",
    "\\end{equation}\n",
    "\n",
    "This will return a vector of length $m$. Each dimension of this vector should correspond to the unnormalized probability $\\tilde{p}_k$, commonly referred to as the _logits_. We then require normalization of the probability output, which can be achieved by dividing by $\\sum_k \\tilde{p}_k$. Hence we have:\n",
    "\\begin{align}\n",
    "    p(y=k \\vert \\mathbf{x}) = \\frac{\\exp\\left[\\left(\\mathbf{w}_k^T \\Phi(\\mathbf{x})\\right)\\right]}{\\sum_n \\exp\\left[\\left(\\mathbf{w}_n^T \\Phi(\\mathbf{x})\\right)\\right]}\n",
    "\\end{align}\n",
    "The process of converting the unnormalized weight-feature dot product(s) to a normalized distribution is called a softmax operation. Since the exponential is monotonic, the class prediction is given by taking the index with the highest conditional probability - i.e. the highest score $\\mathbf{w}_k^T \\mathbf{\\Phi}(\\mathbf{x})$. The classifier is trained by minimizing the negative log likelihood, which corresponds to the negative cross entropy loss.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(\\mathbf{w}) = -\\log \\prod_k p(y=k \\vert \\mathbf{x}; \\mathbf{w}) = -\\sum_k y_k \\log p\\left(y=k \\vert \\mathbf{x}; \\mathbf{w}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "We'll start by using some convenience functions provided by Torch to download the canonical `MNIST` dataset, transform and load it into Tensor format. MNIST is a multi-class classification data set where the instances $\\mathbf{x}$ are images of handwritten digits (28Ã—28 pixels with a single 8-bit channel). Here the target $y_k \\in \\{0, 1, \\ldots, 9\\}$. We'll train in batches of multiple elements to exploit vectorization of matrix operations. \n",
    "\n",
    "The data is already split into training and test sets. The training set contains 60,000 instances and the test set contains 10,000 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainset = torchvision.datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = torchvision.datasets.MNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize 8 randomly sampled digits below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images[:8]\n",
    "labels = labels[:8]\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "#### The following sections are especially important! Please ensure you understand every word below, and bother your tutor if anything is unclear!\n",
    "\n",
    "\n",
    "Let's step through what we need to setup before we can invoke the training loop. The first thing on the agenda is to define our trainable parameters - in this case, the $10$ weight vectors arranged into a `[n_classes, n_features]` matrix and a bias vector of length `n_classes`. We'll use a popular empirically motivated initialization scheme called Xavier/Glorot initialization to use for our initial weight parameters. Note that here we are using each pixel in the image as a feature. This is not the ideal way to process the information contained within a natural image, but we'll stick with it for the sake of simplicity. `MNIST` is so simple that almost any sensible method will yield $<90 \\%$ accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "n_features = 784\n",
    "n_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# W = torch.zeros([n_features, n_classes], requires_grad=True)\n",
    "W = torch.nn.init.xavier_uniform_(torch.empty([n_features, n_classes], requires_grad=True))  # Initialize weight vector\n",
    "b = torch.zeros([n_classes], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the logits (unnormalized probabilities) are defined as the result of the computation $\\tilde{p}_k = W^T \\mathbf{x} + b$, where $\\mathbf{x}$ is the flattened image. This will return a vector of length `n_classes`. Each entry in this vector corresponds to an unnormalized score corresponding to the likelihood of the class index (higher is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = images[0]  # Take a sample iamge\n",
    "x = x.view(-1)  # Flatten the image\n",
    "logits = torch.matmul(torch.t(W),x) + b  # Compute logits\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define the loss function we will use and construct an optimizer that will update the learnable parameters in our model through some gradient-based update scheme to minimize the loss function. In this case just vanilla SGD with a momentum term. We need to supply all the learnable parameters in our model to the `parameters` argument. This is needed for MLE, as we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD([W,b], lr=1e-2) #, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined the learnable parameters, and defined a loss function and optimizer that tells us how to update our parameters to minimize the cross-entropy loss. Now we enter the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_INTERVAL = 250\n",
    "running_loss, running_accuracy = list(), list()\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):  # Loop over training dataset `n_epochs` times\n",
    "    \n",
    "    epoch_loss = 0.\n",
    "    \n",
    "    for i, data in enumerate(train_loader):  # Loop over elements in training set\n",
    "        \n",
    "        x, labels = data\n",
    "        \n",
    "        x = x.view(batch_size, -1)  # Flatten images but keep batch dimension\n",
    "        logits = torch.matmul(x,W) + b  # Compute scores\n",
    "        \n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        train_acc = torch.mean(torch.eq(predictions, labels).float()).item()\n",
    "        \n",
    "        loss = criterion(input=logits, target=labels)\n",
    "    \n",
    "        loss.backward()               # Backward pass (compute parameter gradients)\n",
    "        optimizer.step()              # Update weight parameter using SGD\n",
    "        optimizer.zero_grad()         # Reset gradients to zero for next iteration\n",
    "        \n",
    "        \n",
    "        # ============================================================================\n",
    "        # You can safely ignore the boilerplate code below - just reports metrics over\n",
    "        # training and test sets\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "        running_accuracy.append(train_acc)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if i % LOG_INTERVAL == 0:  # Log training stats\n",
    "            deltaT = time.time() - start_time\n",
    "            mean_loss = epoch_loss / (i+1)\n",
    "            print('[TRAIN] Epoch {} [{}/{}]| Mean loss {:.4f} | Train accuracy {:.5f} | Time {:.2f} s'.format(epoch, \n",
    "                i, len(train_loader), mean_loss, train_acc, deltaT))\n",
    "        \n",
    "    print('Epoch complete! Mean loss: {:.4f}'.format(epoch_loss/len(train_loader)))\n",
    "    \n",
    "    test_loss = 0.\n",
    "    test_preds, test_labels = list(), list()\n",
    "    for i, data in enumerate(test_loader):\n",
    "        x, labels = data\n",
    "        x = x.view(batch_size, -1)  # Flatten images but keep batch dimension\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = torch.matmul(x,W) + b  # Compute scores\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            test_loss += criterion(input=logits, target=labels).item()\n",
    "            test_preds.append(predictions)\n",
    "            test_labels.append(labels)\n",
    "            \n",
    "    test_preds = torch.cat(test_preds)\n",
    "    test_labels = torch.cat(test_labels)\n",
    "\n",
    "    test_accuracy = torch.eq(test_preds, test_labels).float().mean().item()\n",
    "    \n",
    "    print('[TEST] Mean loss {:.4f} | Accuracy {:.4f}'.format(test_loss/len(test_loader), test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be getting $>90/\\%$ train accuracy with similar test accuracy within a minute on CPU _(note to tutors - check on your machine?)_, not bad for a _linear method_! ðŸ˜Ž Finally, let's plot the loss and accuracy curves. You may want to fiddle with the learning rate when your loss stats to plateau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter  # Smooth spiky curves\n",
    "running_loss_smoothed = savgol_filter(running_loss, 21, 3)\n",
    "running_acc_smoothed = savgol_filter(running_accuracy, 21, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_loss_smoothed)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cross-entropy Loss (Train)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_acc_smoothed)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy (Train)')\n",
    "plt.ylim(0.2,1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus:** Can you improve on this? You may want to try building a multilayer Perceptron, changing the optimizer, experimenting with learning rates or momentum, or building a convolutional NN architecture. Some quick modifications should allow you to surpass 98% accuracy pretty easily.\n",
    "\n",
    "That's all for this week. Next week we'll be looking at best practices and more complicated workloads in Pytorch, particularly how to use the canoncial `torch.nn.Module` class to streamline your work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
