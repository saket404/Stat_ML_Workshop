{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Workshop 3\n",
    "## Linear regression\n",
    "\n",
    "***\n",
    "\n",
    "Our aim for this part of the workshop is to fit a linear model from scratchâ€”relying only on the `numpy` library. We'll experiment with analytic solution based on linear algebra first. Then, to check the correctness of our implementation, we'll compare its output to the output of `sklearn`.\n",
    "\n",
    "Firstly we will import the relevant libraries (`numpy`, `matplotlib`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Review\n",
    "In lectures, we saw that a linear model can be expressed as:\n",
    "$$y(\\mathbf{x}, \\mathbf{w}) = w_0 + \\sum_{j = 1}^{m} w_j x_j = \\mathbf{w} \\cdot \\mathbf{x} $$\n",
    "where \n",
    "\n",
    "* $y$ is the *target variable*;\n",
    "* $\\mathbf{x} = [x_1, \\ldots, x_m]$ is a vector of *features* (we define $x_0 = 1$); and\n",
    "* $\\mathbf{w} = [w_0, \\ldots, w_m]$ are the *weights*.\n",
    "\n",
    "To acknowledge the uncertainty inherent in our model, we assume a _probabilistic model_ where the target variable $y$ is given by a deterministic function of the data/weights and additive Gaussian noise with zero mean and variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    y &= \\mathbf{w} \\cdot \\mathbf{x} + \\epsilon \\\\\n",
    "    \\epsilon &\\sim \\mathcal{N}(0, \\sigma^2)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This enables us to model the target variable as:\n",
    "\n",
    "\\begin{equation}\n",
    "    p(y \\vert \\mathbf{x}) = \\prod_k^n \\mathcal{N}(y_k; \\mathbf{w} \\cdot \\mathbf{x}, \\sigma^2)\n",
    "\\end{equation}\n",
    "\n",
    "We maximize the log-likelihood to choose parameter values maximizing the probability of the observed data. We saw in lectures that maximizing the log-likelihood is equivalent to _minimization_ of the following sum of squared residuals loss function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E(\\mathbf{w}) = \\sum_{i=1}^{n}(y_i - \\mathbf{w} \\cdot \\mathbf{x}_i)^2$$\n",
    "\n",
    "**Note:** For simplicity, we'll consider the case $m = 1$ (i.e. only one feature excluding the intercept)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset\n",
    "\n",
    "We'll work with the well-known _Boston Housing Dataset_ containing data about housing suburbs in Boston, Massachuetts. There are 13 features in this dataset which are intended to be used to predict the target, the median house value in the given suburb, `MEDV`. For simplicity we'll only work with a single variable, the percentage of the population in the suburb classified as 'lower status' by the US Census service in 1978, `LSTAT`. Plotting `LSTAT` vs. `MEDV` we see that a linear model appears plausible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_RESOLUTION = 128\n",
    "plt.rcParams['figure.dpi'] = FIGURE_RESOLUTION\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "ds = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "features = ['LSTAT'] #  ds.columns  # ['RM', 'LSTAT']\n",
    "target = 'MEDV'\n",
    "\n",
    "ds['LSTAT'] = ds['LSTAT'].apply(lambda x: x/100.)\n",
    "\n",
    "for f in features:\n",
    "    plt.figure()\n",
    "    plt.scatter(ds[f], boston.target, marker='.')\n",
    "    plt.xlabel(f)\n",
    "    plt.ylabel('Median House Value (thousands)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = boston.target\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_full_train, x_full_test, y_train, y_test = train_test_split(ds, Y, test_size=0.2, random_state=90051)\n",
    "print(\"Training set has {} instances. Test set has {} instances.\".format(x_full_train.shape[0], x_full_test.shape[0]))\n",
    "x = x_full_train[features].values.ravel()\n",
    "x_test = x_full_test[features].values.ravel()\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Linear algebra solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lectures, we saw that it's possible to solve for the optimal weights $\\mathbf{w}^\\star$ analytically by solving for $\\nabla_{\\mathbf{w}} E(\\mathbf{w}) = 0$. This yields the _normal equations_ for the least squares problem:\n",
    "$$\\mathbf{w}^* = \\left[\\mathbf{X}^\\top \\mathbf{X}\\right]^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "where $X$ is the _design matrix_. In our simple 1-feature case this is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{X} = \\begin{pmatrix} \n",
    "        1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \n",
    "    \\end{pmatrix} \n",
    "  \\quad \\text{and} \\quad \n",
    "  \\mathbf{y} = \\begin{pmatrix} \n",
    "          y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
    "      \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack((np.ones_like(x), x))\n",
    "X_test = np.column_stack((np.ones_like(x_test), x_test))\n",
    "print('Design matrix shape:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can express $\\mathbf{w}^\\star$ explicitly in terms of the matrix inverse $(\\mathbf{X}^\\top \\mathbf{X})^{-1}$, this isn't an efficient way to compute $\\mathbf{w}$ numerically. It is better instead to solve the following system of linear equations:\n",
    "$$\\mathbf{X}^\\top\\mathbf{X} \\mathbf{w}^\\star = \\mathbf{X}^\\top\\mathbf{y}$$\n",
    "\n",
    "This can be done in numpy using the command `np.linalg.solve`. Enter `np.linalg.solve?` to see its docstring. Using this method gives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which gives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linalg.solve(np.dot(X.T, X), np.dot(X.T, y))\n",
    "print('Weights:', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, w): \n",
    "    return ...  # fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit(x_test, y_test, x, y):\n",
    "    \n",
    "    # To get connecting lines in matplotlib correct\n",
    "    sort_idx = np.argsort(x_test)\n",
    "    y_test = y_test[sort_idx]\n",
    "    x_test = np.sort(x_test)\n",
    "    plt.plot(x_test, y_test, 'b-')\n",
    "    plt.scatter(x, y, marker='.')\n",
    "    plt.ylabel(\"$y$ (Median House Price)\")\n",
    "    plt.xlabel(\"$x$ (LSTAT)\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_fit(x_test, predict(X_test, w), x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll compute the mean error term over the training and test sets to assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(X, w, y):\n",
    "    return ...  # fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train MSE:', mean_squared_error(X,w,y_train))\n",
    "print('Test MSE:', mean_squared_error(X_test,w,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Solving using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a good understanding of what's going on under the hood, you can use the functionality in `sklearn` to solve linear regression problems you encounter in the future. Using the `LinearRegression` module, fitting a linear regression model becomes a one-liner as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression().fit(np.expand_dims(x,1), np.expand_dims(y,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LinearRegression` module provides access to the bias weight $w_0$ under the `intercept_` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the non-bias weights under the `coef_` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should check that these results match the solution you obtained previously. Note that sklearn also uses a numerical linear algebra solver under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, what happens if we use the other 12 variables available in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_full = LinearRegression().fit(x_full_train, y_train)\n",
    "X_full = np.column_stack((np.ones(shape=x_full_train.shape[0]), x_full_train))\n",
    "X_full_test = np.column_stack((np.ones(shape=x_full_test.shape[0]), x_full_test))\n",
    "w_full = (lr_full.intercept_, *lr_full.coef_)\n",
    "print('Full design matrix shape:', X_full.shape)\n",
    "\n",
    "print('Train MSE:', mean_squared_error(X_full,w_full,y_train))\n",
    "print('Test MSE:', mean_squared_error(X_full_test,w_full,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As measured by the MSE, our predictions are looking better. Although we obtained a performance boost here, in real problems you should be cautious of blindly including features in your analysis just because you can.\n",
    "\n",
    "## 5. Introducing Nonlinear Basis Functions\n",
    "\n",
    "While linear regression is extremely effective in the right context, the formulation above, where we model the target $y = \\mathbf{w} \\cdot \\mathbf{x} + \\epsilon, \\; \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ is a linear function in the variables $\\mathbf{x}$, which may limit the expressiveness of our model. We can increase the flexibility of our model by introducing nonlinear basis functions of the input variables, $\\vec{\\phi}(\\mathbf{x})$ in lieu of the original features $\\mathbf{x}$. Now our predictor $y = \\mathbf{w} \\cdot \\vec{\\phi}(\\mathbf{x})$ is a nonlinear function of the input $\\mathbf{x}$ (but still linear in the model parameters $\\mathbf{w}$). There are many possible choices for the basis $\\vec{\\phi}(\\mathbf{x})$, but we will focus on using polynomial basis functions with the form $\\phi(x) = x^j$ in the single-variable case. \n",
    "\n",
    "All our previous results for linear regression carry over as you would expect, we replace $\\mathbf{w} \\cdot \\mathbf{x}$ with $\\mathbf{w} \\cdot \\vec{\\phi}(\\mathbf{x})$ - the error function and normal equations become:\n",
    "\n",
    "\\begin{equation}\n",
    "    E(\\mathbf{w}) = \\sum_{i=1}^{n}(y_i - \\mathbf{w} \\cdot \\vec{\\phi}(\\mathbf{x}_i))^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{w}^* = \\left[\\mathbf{\\Phi}^\\top \\mathbf{\\Phi}\\right]^{-1} \\mathbf{\\Phi}^\\top \\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "Here the design matrix $\\Phi \\in \\mathbb{R}^{n \\times m}$ for $n$ datapoints and $m$ basis functions, $\\vec{\\phi}(\\mathbf{x}) = (\\phi_0(\\mathbf{x}), \\ldots \\phi_m(\\mathbf{x}))$. In the case of polynomial regression $\\vec{\\phi}(x) = \\left(1,x,x^2,\\ldots, x^m\\right)$ (note the one additional bias parameter). We'll start by building the design matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_matrix_poly(x, x_test, order, silent=True):\n",
    "    \"\"\"\n",
    "    Returns design matrix on training and testing data\n",
    "    for polynomial basis of specified order\n",
    "    \"\"\"\n",
    "    n, n_test, m = x.shape[0], x_test.shape[0], order + 1\n",
    "    Phi = np.zeros([n,m])\n",
    "    Phi_test = np.zeros([n_test,m])\n",
    "\n",
    "    for k in range(order+1):\n",
    "        Phi[:,k] = ...  # fill in\n",
    "        Phi_test[:,k] = ... # fill in \n",
    "\n",
    "    if silent is False:\n",
    "        print('Design matrix shape (training):', Phi.shape)\n",
    "    \n",
    "    return Phi, Phi_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_order = 3\n",
    "Phi, Phi_test = design_matrix_poly(x, x_test, order=start_order, silent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's solve the linear system using the inbuilt `numpy` routine and plot the result of the predicted values on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_poly = np.linalg.solve(...)  # fill in\n",
    "plot_fit(x, predict(Phi, w_poly), x, y)\n",
    "# plot_fit(x_test, predict(Phi_test, w_poly), x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like a better fit than the linear model! Let's take a look at the error terms on the train/test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train MSE for polynomial features of order {}: {:.3f}'.format(start_order, mean_squared_error(Phi, w_poly, y_train)))\n",
    "print('Test MSE for polynomial features of order {}: {:.3f}'.format(start_order, mean_squared_error(Phi_test, w_poly, y_test)))\n",
    "\n",
    "print('Train MSE using linear features only: {:.3f}'.format(mean_squared_error(X, w, y_train)))\n",
    "print('Test MSE using linear features only: {:.3f}'.format(mean_squared_error(X_test, w, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strange, a large reduction on the train MSE but not so much on the test MSE. Lets scan across a range of powers. What do you expect to happen as we increase the maximum polynomial order on the training set? Take a minute to discuss with your fellow students before executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = list(range(12))\n",
    "Phi_test_orders = list()\n",
    "w_orders = list()\n",
    "mse_orders = list()\n",
    "\n",
    "sort_idx = np.argsort(x)\n",
    "y_plot = y[sort_idx]\n",
    "x_plot = np.sort(x)\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "for i, order in enumerate(orders):\n",
    "    sub = plt.subplot(len(orders)//2, 2, i+1)    \n",
    "    Phi, Phi_test = design_matrix_poly(x, x_test, order=order)\n",
    "    w_poly = np.linalg.solve(np.dot(Phi.T, Phi), np.dot(Phi.T, y))\n",
    "    mse = mean_squared_error(Phi, w_poly, y)\n",
    "\n",
    "    Phi_test_orders.append(Phi_test)\n",
    "    w_orders.append(w_poly)\n",
    "    mse_orders.append(mse)\n",
    "    \n",
    "    plt.plot(x_plot, predict(Phi, w_poly)[sort_idx], 'b-')\n",
    "    plt.scatter(x_plot, y_plot, marker='.')\n",
    "    plt.title('Order {} | MSE {:.3f}'.format(order, mse))\n",
    "    plt.autoscale()\n",
    "    \n",
    "plt.suptitle('Predicted target values (train) with different maximum polynomial orders.', y=1.05, fontsize=32)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll repeat on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_test_orders = list()\n",
    "sort_idx = np.argsort(x_test)\n",
    "y_plot = y_test[sort_idx]\n",
    "x_plot = np.sort(x_test)\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "for i, order in enumerate(orders):\n",
    "    sub = plt.subplot(len(orders)//2, 2, i+1)    \n",
    "    Phi_test = Phi_test_orders[i]\n",
    "    w_poly = w_orders[i]\n",
    "    \n",
    "    mse_test = mean_squared_error(Phi_test, w_poly, y_test)\n",
    "    mse_test_orders.append(mse_test)\n",
    "    \n",
    "\n",
    "    plt.plot(x_plot, predict(Phi_test, w_poly)[sort_idx], 'b-')\n",
    "    plt.scatter(x_plot, y_plot, marker='.')\n",
    "    plt.title('Order {} | MSE {:.3f}'.format(order, mse_test))\n",
    "    plt.autoscale()\n",
    "    \n",
    "plt.suptitle('Predicted target values (Test) with different maximum polynomial orders.', y=1.05, fontsize=32)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot MSE vs. polynomial order for the train and held-out datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(mse_orders)), mse_orders, label='Train')\n",
    "plt.plot(range(len(mse_test_orders)), mse_test_orders, label='Test')\n",
    "plt.title('Mean squared error vs. Maximum polynomial order')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Maximum polynomial order')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**: ðŸ¤¨ What is going on here? Does this match your earlier findings, or your intuition about which model order was most appropriate? Why isn't held-out error behaving the same as training error?\n",
    "\n",
    "In principle increasing the polynomial order should increase the 'hypothesis space' of the model. However the MSE on the test set appears to rise with increasing polynomial order past 2. The results we covered above should make you uneasy about increasing model capacity without careful thought. Fortunately Lecture 5 on 'Regularization' and next week's workbook should assuage your concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Kernel Ridge Regression\n",
    "This section is optional: we are unlikely to have time for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several (theoretically well-motivated) approaches suggest minimizing a tradeoff between the empirical squared loss and the $L_2$ norm of the weight vector, or equivalently the norm squared. Kernel ridge regression is defined by minimization of an objective function with the form:\n",
    "\n",
    "\\begin{equation}\n",
    "    E(\\mathbf{w}) = \\sum_{i=1}^{n}(y_i - \\mathbf{w} \\cdot \\vec{\\phi}(\\mathbf{x}_i))^2 + \\frac{\\lambda}{2} \\vert \\vert \\mathbf{w} \\vert \\vert^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we set $\\nabla_{\\mathbf{w}} E(\\mathbf{w}) = 0$ to find the global minimum of the error function. Proceeding as before, we obtain the solution\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{w}^* = \\left(\\mathbf{\\Phi}^\\top \\mathbf{\\Phi} + \\lambda \\mathbb{1}\\right)^{-1} \\mathbf{\\Phi}^\\top \\mathbf{y}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $\\mathbf{\\Phi}^\\top \\mathbf{\\Phi} + \\lambda \\mathbb{1}$ is always invertible (we leave this as an exercise to readers familiar with linear algebra), so ridge regression has a closed-form solution. We'll generate the same plots as above but including this L2 weight penalty term using $\\lambda = 0.001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = list(range(12))\n",
    "Phi_test_orders = list()\n",
    "w_orders = list()\n",
    "mse_orders = list()\n",
    "\n",
    "sort_idx = np.argsort(x)\n",
    "y_plot = y[sort_idx]\n",
    "x_plot = np.sort(x)\n",
    "\n",
    "lambda_RR = 0.001\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "for i, order in enumerate(orders):\n",
    "    sub = plt.subplot(len(orders)//2, 2, i+1)    \n",
    "    Phi, Phi_test = design_matrix_poly(x, x_test, order=order)\n",
    "    w_poly = np.linalg.solve(np.dot(Phi.T, Phi) + lambda_RR * np.eye(order+1) , np.dot(Phi.T, y))\n",
    "    mse = mean_squared_error(Phi, w_poly, y)\n",
    "\n",
    "    Phi_test_orders.append(Phi_test)\n",
    "    w_orders.append(w_poly)\n",
    "    mse_orders.append(mse)\n",
    "    \n",
    "    plt.plot(x_plot, predict(Phi, w_poly)[sort_idx], 'b-')\n",
    "    plt.scatter(x_plot, y_plot, marker='.')\n",
    "    plt.title('Order {} | MSE {:.3f}'.format(order, mse))\n",
    "    plt.autoscale()\n",
    "    \n",
    "plt.suptitle('Predicted target values (train) using ridge regression.', y=1.05, fontsize=32)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_test_orders = list()\n",
    "sort_idx = np.argsort(x_test)\n",
    "y_plot = y_test[sort_idx]\n",
    "x_plot = np.sort(x_test)\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "for i, order in enumerate(orders):\n",
    "    sub = plt.subplot(len(orders)//2, 2, i+1)    \n",
    "    Phi_test = Phi_test_orders[i]\n",
    "    w_poly = w_orders[i]\n",
    "    \n",
    "    mse_test = mean_squared_error(Phi_test, w_poly, y_test)\n",
    "    mse_test_orders.append(mse_test)\n",
    "    \n",
    "\n",
    "    plt.plot(x_plot, predict(Phi_test, w_poly)[sort_idx], 'b-')\n",
    "    plt.scatter(x_plot, y_plot, marker='.')\n",
    "    plt.title('Order {} | MSE {:.3f}'.format(order, mse_test))\n",
    "    plt.autoscale()\n",
    "    \n",
    "plt.suptitle('Predicted target values (test) using ridge regression.', y=1.05, fontsize=32)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(mse_orders)), mse_orders, label='Train')\n",
    "plt.plot(range(len(mse_test_orders)), mse_test_orders, label='Test')\n",
    "plt.title('Mean squared error vs. Maximum polynomial order w/ Ridge Regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Maximum polynomial order')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll plot the L2 norm of the found weights versus max. polynomial order. You should compare this with the non-regularized values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_L2 = [np.sum(w**2) for w in w_orders]\n",
    "plt.plot(range(len(w_L2)), w_L2)\n",
    "plt.xlabel('Maximum polynomial order')\n",
    "plt.ylabel(r'$\\vert \\vert \\mathbf{w} \\vert \\vert^2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Discussion:** You may want to experiment with different settings for $\\lambda$. How would one select the 'best' value of $\\lambda$ to balance between good train performance and generalization?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
